# üèóÔ∏è Transformer From Scratch (Python Puro)

Este reposit√≥rio cont√©m uma implementa√ß√£o educacional completa da arquitetura **Transformer**, constru√≠da exclusivamente em **Python puro** (sem o uso de frameworks como PyTorch ou TensorFlow). O objetivo √© desvendar a "caixa-preta" dos modelos de linguagem modernos (LLMs) atrav√©s da implementa√ß√£o direta das equa√ß√µes do artigo original *[Attention Is All You Need](https://arxiv.org/abs/1706.03762)*.

## üéØ Objetivo do Projeto

Demonstrar a mec√¢nica interna de um Transformer, desde opera√ß√µes de √°lgebra linear b√°sica at√© o fluxo autoregressivo de gera√ß√£o de texto, mantendo a estabilidade num√©rica e a fidelidade matem√°tica.

## üõ†Ô∏è Arquitetura Implementada

O modelo segue a estrutura cl√°ssica de um **Decoder-only** (estilo GPT), apresentando os seguintes componentes:

### 1. Positional Encoding (PE)

Como o Transformer processa tokens em paralelo, ele n√£o possui no√ß√£o intr√≠nseca de ordem. Utilizamos fun√ß√µes senoidais e cossenoidais para injetar informa√ß√µes de posi√ß√£o:

$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$

$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$

### 2. Scaled Dot-Product Attention

O cora√ß√£o do modelo, onde as matrizes de **Query (Q)**, **Key (K)** e **Value (V)** interagem para determinar a relev√¢ncia contextual:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

* **Causal Masking:** Implementa√ß√£o de uma m√°scara triangular superior ($-\infty$) para garantir que o modelo n√£o "olhe para o futuro" durante a gera√ß√£o.

### 3. Layer Normalization & Res√≠duos

Para garantir a estabilidade do treinamento e evitar a degrada√ß√£o do gradiente, aplicamos:

* **Conex√µes Residuais:** $X_{out} = X + \text{Sublayer}(X)$.
* **Layer Norm:** Normaliza√ß√£o por token para manter m√©dia $0$ e vari√¢ncia $1$.

### 4. Feed-Forward Network (FFN)

Uma camada densa aplicada individualmente a cada posi√ß√£o, introduzindo n√£o-linearidade atrav√©s da ativa√ß√£o **ReLU**:

$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

---

## üìä Visualiza√ß√£o e Diagn√≥stico

O projeto inclui ferramentas de visualiza√ß√£o para validar o comportamento do modelo:

* **Matriz de Aten√ß√£o 3D:** Visualiza√ß√£o por "fibras" de luz que demonstram o fluxo de aten√ß√£o causal entre tokens.
* **An√°lise de Loss:** C√°lculo de erro via Cross-Entropy (ou MSE simplificado) para medir a precis√£o da predi√ß√£o em rela√ß√£o a um alvo real. No teste final, obtivemos uma **Loss de 0.7029**, indicando um sistema pronto para o processo de otimiza√ß√£o.

---

## üöÄ Como Executar

O projeto est√° contido em um Jupyter Notebook (`.ipynb`). Basta abrir o arquivo e executar as c√©lulas sequencialmente. N√£o existem depend√™ncias externas al√©m de bibliotecas padr√£o do Python (como `math` e `random`) e `matplotlib/plotly` para as visualiza√ß√µes.

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/Pure-Python-Transformer-Architect.git

# Abra o notebook
jupyter notebook TRANSFORMER(PYTHON_PURO).ipynb

```

## üß† Conclus√£o Acad√™mica

Este projeto prova que, por tr√°s da complexidade de modelos como o GPT-4, existe uma estrutura elegante de matrizes operando em harmonia. A estabilidade num√©rica alcan√ßada (m√©dia de ativa√ß√£o $0.0000$ ap√≥s LayerNorm) confirma que a implementa√ß√£o est√° pronta para receber o algoritmo de *Backpropagation* e evoluir para um treinamento real.

---

**Desenvolvido para fins de estudo profundo em IA e Arquiteturas Neurais.**
